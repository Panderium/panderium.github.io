---
title: "Keda: l‚Äôautoscaler qui va sauver votre production"
date: 2025-03-14T20:15:23+01:00
draft: true
categories: [kubernetes]
tags: [Kubernetes, scaling]
---

Vous avez soigneusement con√ßu une architecture distribu√©e bas√©e sur des microservices, convaincu que cela assurerait la scalabilit√© de vos applications. Mais voil√†, apr√®s la mise en ligne de votre article phare les commandes s‚Äôenchainent mais la performance se fait attendre‚Ä¶ jusqu‚Äôau blackout... Ce qui devait √™tre un grand moment de succ√®s se transforme en v√©ritable casse-t√™te, votre infrastructure est √©t√© incapable de g√©rer l‚Äôarriv√© massive d‚Äôutilisateur sur votre site. Ce sc√©nario aurait sans doute pu √™tre √©vit√© si vous aviez mise en place de **l‚Äôautoscaling** √©v√©nementiel avec **Keda**.

## Les limites de l‚Äôautoscaling ‚Äútraditionnelle‚Äù

**L'autoscaling** est d√©sormais essentiel pour une gestion dynamique des ressources et pour garantir la performance des applications. Cependant, **l'autoscaling** traditionnel reposant sur des m√©triques comme l'utilisation du CPU ou de la m√©moire, peut ne pas √™tre suffisant lorsque la charge des applications est influenc√©e par des facteurs externes. S‚Äôappuyer uniquement sur ces m√©triques peut emp√™cher une application de r√©pondre efficacement √† l‚Äôaugmentation de la demande des utilisateurs. Dans ces situations, il devient crucial d‚Äôadapter le **scaling** des applications en fonction de **m√©triques** plus adapt√©es capables d‚Äôanticiper au mieux les besoins des utilisateurs. Par exemple, une application qui r√©agit √† des √©v√©nements sp√©cifiques ‚Äî tels que des messages dans une file d'attente ou des requ√™tes provenant de services externes ‚Äî n√©cessitera un **autoscaling** plus flexible et r√©actif afin de r√©pondre rapidement aux variations de la demande.

C‚Äôest ici qu‚Äôintervient [**KEDA**](https://keda.sh/) (Kubernetes Event-Driven Autoscaling), un outil pour **Kubernetes** sp√©cialement con√ßu afin **d'autoscaler** en fonction **d‚Äô√©v√©nements**. KEDA permet √† Kubernetes de r√©agir de mani√®re proactive aux √©v√©nements qui affectent les applications, comme l‚Äôarriv√©e de nouveaux messages, l‚Äôajout de donn√©es dans une base de donn√©es ou des m√©triques provenant du cloud, par exemple. Cela permet d'ajuster la capacit√© des *pods* en temps r√©el en fonction de la demande, et ainsi d‚Äô√©viter la surcharge du syst√®me ou le gaspillage de ressources. En d'autres termes, **KEDA** vous aide √† garantir que vos applications soient toujours dimensionn√©es correctement, en fonction des √©v√©nements qu'elles rencontrent. Cet article vous montrera comment installer et configurer KEDA sur Kubernetes pour am√©liorer **l'autoscaling** dans vos environnements de production.

## De la th√©orie √† la pratique

### A. Pr√©sentation de l‚Äôoutil

Si vous √™tes familier avec Kubernetes, vous avez probablement d√©j√† utilis√© les Horizontal Pod Autoscalers (HPA). Keda va plus loin en ajoutant √† Kubernetes un ensemble de Custom Resource Definitions (CRD), permettant d‚Äô√©tendre les capacit√©s d‚Äôautoscaling au-del√† de ce que propose le HPA. En fait, Keda agit comme un serveur de m√©triques qui fournit des donn√©es au HPA (en utilisant les APIs *custom.metrics.k8s.io*¬†et *external.metrics.k8s.io* fournis par Kubernetes). Ces m√©triques sont collect√©es via des [scalers](https://keda.sh/docs/2.16/scalers/), qui sont utilis√©s dans des ressources comme les [ScaledObjects](https://keda.sh/docs/2.16/concepts/scaling-deployments/) (pour les Deployments, StatefulSets et ressources personnalis√©es) ou les [ScaledJobs](https://keda.sh/docs/2.16/concepts/scaling-jobs/) (pour les Jobs).

Un des grands avantages de **Keda** est sa capacit√© √† mettre un *Deployment* √† l'√©chelle de z√©ro, ce qui est particuli√®rement utile dans une optique de **FinOps** et de **GreenIT** üòä.

Pour en savoir plus sur le fonctionnement interne de Keda je vous invite √† [visiter cette documentation](https://keda.sh/docs/2.16/concepts/).

### B. HPA vs Keda

| **Crit√®re** | **Autoscaling bas√© sur la charge (HPA)** | **Autoscaling √©v√©nementiel (Keda)** |
| --- | --- | --- |
| **M√©triques utilis√©es**¬†
¬†
 | CPU, m√©moire, ou toute autre m√©trique d√©finie par l'utilisateur (mais souvent limit√©e √† celles disponibles sur les pods). | Toute m√©trique ou √©v√©nement provenant de sources internes ou externes comme RabbitMQ, Kafka, Azure Service Bus, etc. |
| **R√©activit√©**¬†
¬†
 | Moins r√©actif aux √©v√©nements en temps r√©el. L'autoscaling se d√©clenche apr√®s un certain seuil d'utilisation de ressources.¬†
¬†
 | Tr√®s r√©actif aux √©v√©nements en temps r√©el, permettant un ajustement imm√©diat en fonction de la demande. |
| **Flexibilit√©**¬†
¬†
 | Limit√©e √† des m√©triques internes et bien d√©finies (comme CPU/m√©moire).

Impl√©menter soit m√™me des m√©triques customs peut s‚Äôav√©rer p√©rilleux.¬†
 | Tr√®s flexible, permet d‚Äôint√©grer une grande vari√©t√© de sources et de types d‚Äô√©v√©nements pour d√©clencher l‚Äôautoscaling. |
| **Gestion du scaling √† z√©ro** | Impossible, il reste toujours au minimum un pod. | Keda permet de scaler √† z√©ro, utile dans des sc√©narios o√π des ressources qui doivent √™tre lib√©r√©es lorsque la demande est nulle. |
| **Prise en charge des √©v√©nements externes** | N√©cessite une impl√©mentation de l‚Äôapi *external.metrics.k8s.io* | Prise en charge native des √©v√©nements externes via des scalers (ex : RabbitMQ, Kafka, etc.). |

### C. Mise en pratique

Pour tester son fonctionnement, j‚Äôai effectu√© un PoC mettant en ≈ìuvre un *ScaledJob* √©coutant une queue d‚Äôun cluster **RabbitMQ**. N‚Äô√©tant pas expert en programmation, le *job* en question sera un simple serveur Nginx mais libre √† vous d‚Äôimpl√©menter une application consommant les messages de la queue.

Les pr√©requis:

- Un cluster Kubernetes
- Helm
- Kubectl

**1. *Installation de RabbitMQ***

Plusieurs op√©rateurs existent, pour des raisons de simplicit√© j‚Äôai pris celui disponible sur le [Github de RabbitMQ.](https://github.com/rabbitmq/cluster-operator)

**a. Installation de l‚Äôop√©rateur**

kubectl apply -f [https://github.com/rabbitmq/clusteroperator/releases/latest/download/cluster-operator.yml](https://github.com/rabbitmq/cluster-operator/releases/latest/download/cluster-operator.yml)

**b. Installation du Cluster**

Le manifeste suivant cr√©e un cluster **RabbitMQ** dans un nouveau *NameSpace*. Le *ConfigMap* permet d‚Äôinjecter une configuration cr√©ant une queue au d√©marrage de **RabbitMQ**.

```yaml
apiVersion: v1
kind: namespace
metadata:
  name: rabbitmq-cluster
---
apiVersion: rabbitmq.com/v1beta1
kind: RabbitmqCluster
metadata:
  name: rmq-cluster
  namespace: rabbitmq-cluster
spec:
  resources:
    requests:
      cpu: 500m
      memory: 1Gi
    limits:
      cpu: 800m
      memory: 1Gi
  override:
    statefulSet:
      spec:
        template:
          spec:
            containers:
            - name: rabbitmq
              volumeMounts:
              - mountPath: /etc/rabbitmq/definitions.json
                subPath: definitions.json
                name: definitions
            volumes:
            - name: definitions
              configMap:
                name: rmq-cluster-definitions
  rabbitmq:
    additionalConfig: |
      load_definitions = /etc/rabbitmq/definitions.json
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: rmq-cluster-definitions
  namespace: rabbitmq-cluster
data:
  definitions.json: |
    {
      "vhosts": [
        {
          "name": "/"
        }
      ],
      "topic_permissions": [],
      "parameters": [],
      "queues": [
        {
          "name": "keda",
          "vhost": "/",
          "durable": true,
          "auto_delete": false,
          "arguments": {
            "x-queue-type": "classic"
          }
        }
      ]
    }
```

**c. Ajout d‚Äôun utilisateur**

Afin de permettre √† **Keda** de se connecter au cluster j‚Äôai cr√©√© un utilisateur auquel je donne les droits d‚Äôadministration.

```bash
// apr√®s avoir ouvert un shell sur le cluster
$ rabbitmqctl add_user keda keda
$ rabbitmqctl set_user_tags keda administrator
$ rabbitmqctl set_permissions -p / keda ".*" ".*" ".*"
```

***2. Installer Keda***

**a. Installation de l‚Äôop√©rateur**

Rien de plus simple en suivant la documentation¬†[Deploying KEDA | KEDA.](https://keda.sh/docs/2.16/deploy/)

```bash
helm repo add kedacore https://kedacore.github.io/charts
helm repo update
helm install keda kedacore/keda --namespace keda -‚Äìcreate-namespace
```

**b. D√©ploiement d‚Äôun *ScaledJob***

Ajout du *ScaledJob* avec un trigger utilisant le [scaler RabbitMQ](https://keda.sh/docs/2.16/scalers/rabbitmq-queue/).

Notez que j‚Äôutilise un secret que j‚Äôimporte dans le *ScaledJob* pour pouvoir authentifier le *Scaler* √† mon cluster **RabbitMQ**.¬†Bien que cela soit fonctionnel **Keda** propose une gestion bien plus pouss√©e de l‚Äôauthentification avec des ressources de [type *TriggerAuthentication* et *ClusterTriggerAuthentication.*](https://keda.sh/docs/2.16/concepts/authentication/#re-use-credentials-and-delegate-auth-with-triggerauthentication)

```yaml
apiVersion: v1
kind: Secret
metadata:
  name: rabbitmq
  namespace: default
data:
  RABBITMQ_HOST: YW1xcDovL3JtcS1jbHVzdGVyLnJhYmJpdG1xLWNsdXN0ZXI6NTY3Mi92aG9zdA== # amqp://rmq-cluster.rabbitmq-cluster:5672/vhost
  RABBITMQ_USERNAME: a2VkYQ== # keda en base64
  RABBITMQ_PASSWORD: a2VkYQ== # keda en base64
---
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: nginx-keda
  namespace: default
spec:
  jobTargetRef:
    template:
      spec:
        containers:
        - name: nginx
          image: nginx
          imagePullPolicy: Always
          command: ["/usr/bin/bash", "-c", "sleep 30"]
          envFrom:
            - secretRef:
                name: rabbitmq
        restartPolicy: Never
    backoffLimit: 4  
  triggers:
  - type: rabbitmq
    metadata:
      queueName: keda
      mode: QueueLength
      value  : '1'
      vhostName: /
      hostFromEnv: RABBITMQ_HOST
      usernameFromEnv: RABBITMQ_USERNAME
      passwordFromEnv: RABBITMQ_PASSWORD
      unsafeSsl: "true"
```

**3. *D√©monstration***

![demo-keda.gif](/static/demo-keda.gif)

## Conclusion

√Ä travers cette d√©monstration, il est clair que l‚Äôimpl√©mentation de Keda sur un cluster Kubernetes est relativement simple. En s‚Äôappuyant sur le serveur de m√©triques Kubernetes, l‚Äô√©quipe de Keda nous offre une interface unifi√©e permettant de mettre en place une strat√©gie d‚Äôautoscaling bas√©e sur des √©v√©nements. Cette solution l√©g√®re couvre une large gamme de source √† partir desquels on pourrait envisager le scaling. De plus, il est tout √† fait possible de [contribuer √† l‚Äôajout de nouveaux scalers](https://github.com/kedacore/keda/blob/main/CONTRIBUTING.md#contributing-scalers).

Si vous utilisez d√©j√† Kubernetes et que vous cherchez √† am√©liorer la gestion de la charge de vos applications, Keda m√©rite clairement votre attention. Avez-vous d√©j√† test√© cet outil en production ? N‚Äôh√©sitez pas √† partager vos retours d‚Äôexp√©rience et vos cas d‚Äôusage !

## Et si vous passiez √† l‚Äô√©chelle avec ACENSI ?

Chez **ACENSI**, nous accompagnons nos clients dans l‚Äôoptimisation de leurs infrastructures cloud et Kubernetes. Gr√¢ce √† notre expertise en **DevOps** nous aidons les entreprises √† tirer pleinement parti des solutions comme Keda pour garantir une infrastructure r√©active, performante et √©conomique. Vous souhaitez impl√©menter une strat√©gie d‚Äôautoscaling efficace et adapt√©e √† vos besoins ? **Nos experts sont l√† pour vous conseiller et vous accompagner**. N‚Äôh√©sitez pas √† nous contacter pour en discuter !